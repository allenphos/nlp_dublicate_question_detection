# Duplicate Question Detection with NLP

A machine learning project to identify semantically duplicate questions using the Quora dataset. Combines classical ML models and fine-tuned transformer-based architectures.

---

## Project Goal

Classify whether two questions are duplicates by analyzing their semantic similarity. The project explores traditional machine learning, deep learning, and transfer learning methods for natural language understanding.

---

## ğŸ“‘ Table of Contents

- [ğŸ¯ Project Goal](#project-goal)
- [ğŸ“ Project Structure](#project-structure)
- [ğŸ§° Tools & Technologies](#tools--technologies)
- [ğŸ§ª Methodology](#methodology)
- [ğŸ“Š Models & Performance](#models--performance)
- [âœ… Evaluation](#evaluation)
- [ğŸ“¦ Data Files](#project-structure)
- [ğŸ“œ Requirements](#requirements)
- [ğŸ”š Conclusion](#conclusion)
- [ğŸ“ˆ Next Steps](#next-steps)
- [ğŸ‘©â€ğŸ’» Author](#author)

---

## Tools & Technologies

- **Python** â€” core programming language used for all scripts and modeling
- **Jupyter Notebooks** â€” for interactive development, prototyping, and documentation
- **Scikit-learn** â€” classical ML models, metrics, and pipelines
- **Pandas / NumPy** â€” data manipulation and numerical operations
- **TensorFlow / PyTorch** â€” deep learning frameworks for custom model training
- **Hugging Face Transformers** â€” used for fine-tuning and deploying DistilBERT (LLM)
- **DistilBERT (via Transformers)** â€” lightweight transformer model fine-tuned for semantic similarity
- **Large Language Models (LLMs)** â€” transfer learning from pre-trained transformer models
- **Joblib** â€” efficient serialization of trained models for later reuse
- Project functionality was modularized using Object-Oriented Programming (OOP) principles to enhance reusability, scalability, and maintainability of preprocessing, feature engineering, and model training components.

---

## Project Structure

<pre lang="markdown"><code>## ğŸ“ Project Structure ``` 
  
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ processed/
â”‚ â”‚ â”œâ”€â”€ *.zip, *.csv â† Cleaned & augmented datasets
â”‚ â”œâ”€â”€ raw/ â† (*.zip raw Quora files)
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_eda.ipynb â† Exploratory Data Analysis
â”‚ â”œâ”€â”€ 02_preprocessing_and_feature_eng.ipynb â† Cleaning & feature generation
â”‚ â”œâ”€â”€ 03_modeling_and_evaluation.ipynb â† Baseline models: TF-IDF, LogReg, RF
â”‚ â”œâ”€â”€ 04_augmentation.ipynb â† Augmented paraphrases
â”‚ â”œâ”€â”€ 05_modeling_after_augm.ipynb â† Models after data augmentation
â”‚ â”œâ”€â”€ 06_lstm_and_bert_fine-tuning.ipynb â† LSTM and fine-tuned DistilBERT
â”‚ â”œâ”€â”€ 07_evaluation.ipynb â† Final performance & confusion matrices
â”‚
â”œâ”€â”€ reports/
â”‚ â””â”€â”€ metrics_and_confusions.json â† All evaluation scores and confusion matrices
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ analysis/ â† Custom metrics, visualization
â”‚ â”œâ”€â”€ augmentation/ â† Data augmentation scripts
â”‚ â”œâ”€â”€ features/ â† Feature engineering modules
â”‚ â”œâ”€â”€ models/ â† Model training and saving
â”‚ â”œâ”€â”€ preprocessing/ â† Cleaning, tokenization
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md ``` </code></pre>

---

## Models & Performance

| **Model**                                  | **Accuracy** | **F1 Score** | **Log Loss** | **Notes**                                                |
|-------------------------------------------|--------------|--------------|--------------|-----------------------------------------------------------|
| BERT (test set)                            | 0.90         | 0.87         | 0.33         | Best real-world performance, strong generalization        |
| BERT (train set)                           | 0.97         | 0.96         | 0.10         | Extremely high accuracy, but overfitted                   |
| LSTM (sample)                              | 0.78         | 0.68         | 0.46         | Lightweight but weaker, lower recall                     |
| LogReg (TF-IDF+BERT)                       | 0.75         | 0.70         | 0.49         | Strong baseline with interpretable weights               |
| RandomForest (TF-IDF+BERT, before augm)    | 0.82         | 0.74         | 0.42         | Best classic ML model                                    |
| XGBoost (TF-IDF+BERT)                      | 0.73         | 0.69         | 0.50         | Good recall but high false positives                     |
| Ensemble (TF-IDF+BERT)                     | 0.79         | 0.74         | 0.44         | Improved generalization via ensembling                   |
| RandomForest (TF-IDF+BERT, after augm)     | 0.79         | 0.70         | 0.50         | Slight performance drop from noisy augmentation          |

---

## Methodology

- Exploratory Data Analysis (EDA)
- Text preprocessing and feature engineering
- TF-IDF and BERT embeddings
- Data augmentation and balancing
- Classical ML: Logistic Regression, Random Forest, XGBoost
- Deep learning: LSTM with embeddings
- Transformer fine-tuning: DistilBERT
- Model evaluation: accuracy, F1-score, log loss, confusion matrices

---

## Evaluation

- Metrics: Accuracy, F1, Log Loss
- Tools: `classification_report`, `confusion_matrix`, `joblib`, `numpy`
- Final performance summary saved in `metrics_and_confusions.json`

---

## Requirements

Install all dependencies with:

```bash
pip install -r requirements.txt
```
---

## Conclusion

This project demonstrated the effectiveness of combining classical machine learning and modern transformer models for detecting semantically similar questions. Fine-tuned DistilBERT achieved the best results, confirming the power of transfer learning in NLP tasks.

### Key Takeaways:

- **BERT-based models** significantly outperform traditional models in both precision and recall.
- **Classical ML models** remain strong baselines, especially with TF-IDF and engineered BERT features.
- **Augmentation** helps expand training data but may introduce noise if not carefully managed.
- **Model evaluation** with confusion matrices and log loss provided a deep look into performance beyond accuracy.

---
## Next Steps:

- ğŸ“Œ Deploy the best-performing BERT model via a simple API (Streamlit).
- ğŸ§ª Explore other transformer architectures (e.g., RoBERTa, DeBERTa) for further performance gains.
- ğŸ“ˆ Conduct error analysis to identify recurring patterns in false positives/negatives.
- ğŸ“¦ Package the pipeline into a modular, reusable framework for future NLP tasks.
- âš™ï¸ Implement regularization techniques (such as Dropout, Early Stopping, or L2) to reduce overfitting and improve model generalization.

---

## Author

Anastasia Alyoshkina
ğŸ”— [LinkedIn]([https://www.linkedin.com/in/your-link](https://www.linkedin.com/in/anastasiia-alyoshkina-68ba5929a/)) | ğŸ“¬ anastasia.alshkn@gmail.com

---

