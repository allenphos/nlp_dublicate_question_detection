{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6d34c6",
   "metadata": {},
   "source": [
    "This notebook evaluates a range of machine learning models for duplicate question detection on the Quora dataset using both traditional and modern feature representations.\n",
    "Models—including logistic regression, random forest, XGBoost, and a soft-voting ensemble methods—are trained on TF-IDF features, BERT-based similarity features, and their combinations.\n",
    "Each model’s performance is assessed using metrics such as F1-score, log loss, and confusion matrix to ensure robust comparison. Feature importance analyses are performed to interpret the contribution of different features, specifically to compare the predictive value of classic (TF-IDF) and semantic (BERT) features.\n",
    "This systematic evaluation provides actionable insights into model effectiveness and feature utility, guiding the selection of the most robust approach for duplicate detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b9489",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc26db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import joblib\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.features.similarity import TextEmbedder, TextSimilarity\n",
    "from src.models.trainers import ClassicMLTrainer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff92a1a",
   "metadata": {},
   "source": [
    "# Load processed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a06bdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((322840, 9), (80843, 9))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load saved dataframes with precomputed features\n",
    "df_train = pd.read_csv(\"../data/processed/quora_train_with_bert_sim.csv\")\n",
    "df_test = pd.read_csv(\"../data/processed/quora_test_with_bert_sim.csv\")\n",
    "\n",
    "df_train.dropna(inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "df_train.shape, df_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93113796",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = joblib.load(\"../src/models/tfidf_vectorizer.joblib\")\n",
    "\n",
    "# Transform cleaned text columns to TF-IDF feature matrices\n",
    "X_train_tfidf = tfidf_vectorizer.transform(df_train[\"cleaned_text\"])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test[\"cleaned_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfea6b9",
   "metadata": {},
   "source": [
    "# Prepare BERT cosine similarity feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a8105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are dense arrays, need to be reshaped for compatibility with hstack (must be 2D, not 1D)\n",
    "X_train_cos = df_train[['bert_cosine_similarity']].values  # shape: (n_samples, 1)\n",
    "X_test_cos = df_test[['bert_cosine_similarity']].values    # shape: (n_samples, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4333b8",
   "metadata": {},
   "source": [
    "# Combine features for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b46e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate TF-IDF sparse matrix and cosine similarity dense column into a single feature matrix\n",
    "X_train_combined = hstack([X_train_tfidf, X_train_cos])\n",
    "X_test_combined = hstack([X_test_tfidf, X_test_cos])\n",
    "\n",
    "# Target variable\n",
    "y_train = df_train[\"is_duplicate\"].values\n",
    "y_test = df_test[\"is_duplicate\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef318b",
   "metadata": {},
   "source": [
    "# Train classical ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c344830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogReg (TF-IDF)\n",
      "F1-score: 0.6614817091915155\n",
      "Log loss: 0.5354751673520033\n",
      "Confusion matrix:\n",
      " [[37301 13694]\n",
      " [ 8330 21518]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77     50995\n",
      "           1       0.61      0.72      0.66     29848\n",
      "\n",
      "    accuracy                           0.73     80843\n",
      "   macro avg       0.71      0.73      0.72     80843\n",
      "weighted avg       0.74      0.73      0.73     80843\n",
      "\n",
      "com: -5.7108\n",
      "quickbook: 5.4210\n",
      "sahara: 4.8227\n",
      "grad: -4.7978\n",
      "employe: -4.7780\n",
      "abdomin: -4.3213\n",
      "edmonton: 4.2891\n",
      "pride: -4.0329\n",
      "taffi: 3.9205\n",
      "curb: 3.8340\n",
      "\n",
      "RandomForest (TF-IDF)\n",
      "F1-score: 0.7202475749239902\n",
      "Log loss: 0.457589788942944\n",
      "Confusion matrix:\n",
      " [[45486  5509]\n",
      " [ 9949 19899]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85     50995\n",
      "           1       0.78      0.67      0.72     29848\n",
      "\n",
      "    accuracy                           0.81     80843\n",
      "   macro avg       0.80      0.78      0.79     80843\n",
      "weighted avg       0.81      0.81      0.81     80843\n",
      "\n",
      "Feature 708: 0.0143\n",
      "Feature 5690: 0.0087\n",
      "Feature 2886: 0.0064\n",
      "Feature 7772: 0.0058\n",
      "Feature 4081: 0.0057\n",
      "Feature 3481: 0.0054\n",
      "Feature 4092: 0.0052\n",
      "Feature 7532: 0.0051\n",
      "Feature 4011: 0.0046\n",
      "Feature 2944: 0.0042\n",
      "\n",
      "RandomForest (TF-IDF+BERT)\n",
      "F1-score: 0.7408400515109285\n",
      "Log loss: 0.4157966333337342\n",
      "Confusion matrix:\n",
      " [[45154  5841]\n",
      " [ 8850 20998]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86     50995\n",
      "           1       0.78      0.70      0.74     29848\n",
      "\n",
      "    accuracy                           0.82     80843\n",
      "   macro avg       0.81      0.79      0.80     80843\n",
      "weighted avg       0.82      0.82      0.82     80843\n",
      "\n",
      "Feature 8000: 0.1297\n",
      "Feature 708: 0.0112\n",
      "Feature 5690: 0.0071\n",
      "Feature 2886: 0.0053\n",
      "Feature 7772: 0.0052\n",
      "Feature 3481: 0.0047\n",
      "Feature 4081: 0.0046\n",
      "Feature 7532: 0.0043\n",
      "Feature 4092: 0.0042\n",
      "Feature 3447: 0.0041\n",
      "\n",
      "LogReg (TF-IDF+BERT)\n",
      "F1-score: 0.6992122188916838\n",
      "Log loss: 0.4943549745710451\n",
      "Confusion matrix:\n",
      " [[37664 13331]\n",
      " [ 6638 23210]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.74      0.79     50995\n",
      "           1       0.64      0.78      0.70     29848\n",
      "\n",
      "    accuracy                           0.75     80843\n",
      "   macro avg       0.74      0.76      0.74     80843\n",
      "weighted avg       0.77      0.75      0.76     80843\n",
      "\n",
      "bert_cosine_similarity: 24.0564\n",
      "employe: -6.4466\n",
      "quickbook: 5.9698\n",
      "grad: -5.6324\n",
      "com: -5.5943\n",
      "visitor: -4.9239\n",
      "sahara: 4.7797\n",
      "what: 4.5168\n",
      "infinit: 4.1871\n",
      "edmonton: 4.1270\n",
      "\n",
      "XGBoost (TF-IDF+BERT)\n",
      "F1-score: 0.6924198250728864\n",
      "Log loss: 0.5046280051450243\n",
      "Confusion matrix:\n",
      " [[35096 15899]\n",
      " [ 5623 24225]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.69      0.77     50995\n",
      "           1       0.60      0.81      0.69     29848\n",
      "\n",
      "    accuracy                           0.73     80843\n",
      "   macro avg       0.73      0.75      0.73     80843\n",
      "weighted avg       0.77      0.73      0.74     80843\n",
      "\n",
      "Feature 7480: 0.0082\n",
      "Feature 1599: 0.0072\n",
      "Feature 4250: 0.0063\n",
      "Feature 1026: 0.0058\n",
      "Feature 4859: 0.0055\n",
      "Feature 1396: 0.0052\n",
      "Feature 8000: 0.0048\n",
      "Feature 2295: 0.0047\n",
      "Feature 4526: 0.0039\n",
      "Feature 5464: 0.0036\n"
     ]
    }
   ],
   "source": [
    "# Initialize the trainer\n",
    "trainer = ClassicMLTrainer(n_jobs=-1)\n",
    "\n",
    "# Logistic Regression: only TF-IDF features\n",
    "model_tfidf = trainer.train_logreg_tfidf(X_train_tfidf, y_train)\n",
    "trainer.evaluate(model_tfidf, X_test_tfidf, y_test, model_name=\"LogReg\", feature_set=\"TF-IDF\")\n",
    "trainer.feature_importance(model_tfidf, tfidf_vectorizer)\n",
    "\n",
    "# Random Forest: TF-IDF only\n",
    "model_rf_tfidf = trainer.train_rf_tfidf(X_train_tfidf, y_train)\n",
    "trainer.evaluate(model_rf_tfidf, X_test_tfidf, y_test, model_name=\"RandomForest\", feature_set=\"TF-IDF\")\n",
    "trainer.feature_importance(model_rf_tfidf, tfidf_vectorizer)\n",
    "\n",
    "\n",
    "# Random Forest: TF-IDF + BERT\n",
    "model_rf = trainer.train_rf_combined(X_train_combined, y_train)\n",
    "trainer.evaluate(model_rf, X_test_combined, y_test, model_name=\"RandomForest\", feature_set=\"TF-IDF+BERT\")\n",
    "trainer.feature_importance(model_rf, tfidf_vectorizer, feature_names_extra=[\"bert_cosine_similarity\"])\n",
    "\n",
    "\n",
    "# Logistic Regression: TF-IDF + BERT cosine similarity (combined features)\n",
    "model_combined = trainer.train_logreg_combined(X_train_combined, y_train)\n",
    "trainer.evaluate(model_combined, X_test_combined, y_test, model_name=\"LogReg\", feature_set=\"TF-IDF+BERT\")\n",
    "trainer.feature_importance(model_combined, tfidf_vectorizer, feature_names_extra=[\"bert_cosine_similarity\"])\n",
    "\n",
    "# XGBoost: TF-IDF + BERT cosine similarity\n",
    "scale_pos_weight = np.bincount(y_train)[0] / np.bincount(y_train)[1]\n",
    "model_xgb = trainer.train_xgb_combined(X_train_combined, y_train, scale_pos_weight)\n",
    "trainer.evaluate(model_xgb, X_test_combined, y_test, model_name=\"XGBoost\", feature_set=\"TF-IDF+BERT\")\n",
    "trainer.feature_importance(model_xgb, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a66d2",
   "metadata": {},
   "source": [
    "((322840, 9), (80843, 9))\n",
    "\n",
    "LogReg (TF-IDF)\n",
    "F1-score: 0.6614817091915155\n",
    "Log loss: 0.5354751673520033\n",
    "Confusion matrix:\n",
    " [[37301 13694]\n",
    " [ 8330 21518]]\n",
    "Classification report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.82      0.73      0.77     50995\n",
    "           1       0.61      0.72      0.66     29848\n",
    "\n",
    "    accuracy                           0.73     80843\n",
    "   macro avg       0.71      0.73      0.72     80843\n",
    "weighted avg       0.74      0.73      0.73     80843\n",
    "\n",
    "com: -5.7108\n",
    "quickbook: 5.4210\n",
    "sahara: 4.8227\n",
    "grad: -4.7978\n",
    "employe: -4.7780\n",
    "abdomin: -4.3213\n",
    "edmonton: 4.2891\n",
    "pride: -4.0329\n",
    "taffi: 3.9205\n",
    "curb: 3.8340\n",
    "\n",
    "RandomForest (TF-IDF)\n",
    "F1-score: 0.7202475749239902\n",
    "Log loss: 0.457589788942944\n",
    "Confusion matrix:\n",
    " [[45486  5509]\n",
    " [ 9949 19899]]\n",
    "Classification report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.82      0.89      0.85     50995\n",
    "           1       0.78      0.67      0.72     29848\n",
    "\n",
    "    accuracy                           0.81     80843\n",
    "   macro avg       0.80      0.78      0.79     80843\n",
    "weighted avg       0.81      0.81      0.81     80843\n",
    "\n",
    "Feature 708: 0.0143\n",
    "Feature 5690: 0.0087\n",
    "Feature 2886: 0.0064\n",
    "Feature 7772: 0.0058\n",
    "Feature 4081: 0.0057\n",
    "Feature 3481: 0.0054\n",
    "Feature 4092: 0.0052\n",
    "Feature 7532: 0.0051\n",
    "Feature 4011: 0.0046\n",
    "Feature 2944: 0.0042\n",
    "\n",
    "RandomForest (TF-IDF+BERT)\n",
    "F1-score: 0.7408400515109285\n",
    "Log loss: 0.4157966333337342\n",
    "Confusion matrix:\n",
    " [[45154  5841]\n",
    " [ 8850 20998]]\n",
    "Classification report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.89      0.86     50995\n",
    "           1       0.78      0.70      0.74     29848\n",
    "\n",
    "    accuracy                           0.82     80843\n",
    "   macro avg       0.81      0.79      0.80     80843\n",
    "weighted avg       0.82      0.82      0.82     80843\n",
    "\n",
    "Feature 8000: 0.1297\n",
    "Feature 708: 0.0112\n",
    "Feature 5690: 0.0071\n",
    "Feature 2886: 0.0053\n",
    "Feature 7772: 0.0052\n",
    "Feature 3481: 0.0047\n",
    "Feature 4081: 0.0046\n",
    "Feature 7532: 0.0043\n",
    "Feature 4092: 0.0042\n",
    "Feature 3447: 0.0041\n",
    "\n",
    "LogReg (TF-IDF+BERT)\n",
    "F1-score: 0.6992122188916838\n",
    "Log loss: 0.4943549745710451\n",
    "Confusion matrix:\n",
    " [[37664 13331]\n",
    " [ 6638 23210]]\n",
    "Classification report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.85      0.74      0.79     50995\n",
    "           1       0.64      0.78      0.70     29848\n",
    "\n",
    "    accuracy                           0.75     80843\n",
    "   macro avg       0.74      0.76      0.74     80843\n",
    "weighted avg       0.77      0.75      0.76     80843\n",
    "\n",
    "bert_cosine_similarity: 24.0564\n",
    "employe: -6.4466\n",
    "quickbook: 5.9698\n",
    "grad: -5.6324\n",
    "com: -5.5943\n",
    "visitor: -4.9239\n",
    "sahara: 4.7797\n",
    "what: 4.5168\n",
    "infinit: 4.1871\n",
    "edmonton: 4.1270\n",
    "\n",
    "XGBoost (TF-IDF+BERT)\n",
    "F1-score: 0.6924198250728864\n",
    "Log loss: 0.5046280051450243\n",
    "Confusion matrix:\n",
    " [[35096 15899]\n",
    " [ 5623 24225]]\n",
    "Classification report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.86      0.69      0.77     50995\n",
    "           1       0.60      0.81      0.69     29848\n",
    "\n",
    "    accuracy                           0.73     80843\n",
    "   macro avg       0.73      0.75      0.73     80843\n",
    "weighted avg       0.77      0.73      0.74     80843\n",
    "\n",
    "Feature 7480: 0.0082\n",
    "Feature 1599: 0.0072\n",
    "Feature 4250: 0.0063\n",
    "Feature 1026: 0.0058\n",
    "Feature 4859: 0.0055\n",
    "Feature 1396: 0.0052\n",
    "Feature 8000: 0.0048\n",
    "Feature 2295: 0.0047\n",
    "Feature 4526: 0.0039\n",
    "Feature 5464: 0.0036"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add09cc",
   "metadata": {},
   "source": [
    "# Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b78d90e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF and BERT cosine similarity:\n",
    "# Prepare models fitted on combined features\n",
    "lr = trainer.train_logreg_combined(X_train_combined, y_train)\n",
    "rf = trainer.train_rf_combined(X_train_combined, y_train)\n",
    "xgb = trainer.train_xgb_combined(X_train_combined, y_train, scale_pos_weight)\n",
    "\n",
    "estimators = [\n",
    "    ('lr', lr),\n",
    "    ('rf', rf),\n",
    "    ('xgb', xgb)\n",
    "]\n",
    "\n",
    "# Train the ensemble\n",
    "ensemble_model = trainer.train_ensemble(estimators, X_train_combined, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b33c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models\n",
    "trainer.models[\"logreg_tfidf\"] = model_tfidf\n",
    "trainer.models[\"logreg_combined\"] = model_combined\n",
    "trainer.models[\"xgb_combined\"] = model_xgb\n",
    "trainer.models[\"rf_combined\"] = model_rf  \n",
    "trainer.models[\"ensemble_lr\"] = lr\n",
    "trainer.models[\"ensemble_rf\"] = rf\n",
    "trainer.models[\"ensemble_xgb\"] = xgb\n",
    "trainer.models[\"ensemble_model\"] = ensemble_model\n",
    "\n",
    "trainer.save_model(\"logreg_tfidf\", \"../src/models/logreg_tfidf.joblib\")\n",
    "trainer.save_model(\"logreg_combined\", \"../src/models/logreg_combined.joblib\")\n",
    "trainer.save_model(\"xgb_combined\", \"../src/models/xgb_combined.joblib\")\n",
    "trainer.save_model(\"rf_combined\", \"../src/models/rf_combined.joblib\") \n",
    "trainer.save_model(\"ensemble_lr\", \"../src/models/ensemble_lr.joblib\")\n",
    "trainer.save_model(\"ensemble_rf\", \"../src/models/ensemble_rf.joblib\")\n",
    "trainer.save_model(\"ensemble_xgb\", \"../src/models/ensemble_xgb.joblib\")\n",
    "trainer.save_model(\"ensemble_model\", \"../src/models/ensemble_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf553b",
   "metadata": {},
   "source": [
    "\n",
    "# Summary table of results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c357ff9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogReg (TF-IDF)\n",
      "F1-score: 0.6614817091915155\n",
      "Log loss: 0.5354751673520033\n",
      "Confusion matrix:\n",
      " [[37301 13694]\n",
      " [ 8330 21518]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77     50995\n",
      "           1       0.61      0.72      0.66     29848\n",
      "\n",
      "    accuracy                           0.73     80843\n",
      "   macro avg       0.71      0.73      0.72     80843\n",
      "weighted avg       0.74      0.73      0.73     80843\n",
      "\n",
      "\n",
      "LogReg (TF-IDF+BERT)\n",
      "F1-score: 0.6992122188916838\n",
      "Log loss: 0.4943549745710451\n",
      "Confusion matrix:\n",
      " [[37664 13331]\n",
      " [ 6638 23210]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.74      0.79     50995\n",
      "           1       0.64      0.78      0.70     29848\n",
      "\n",
      "    accuracy                           0.75     80843\n",
      "   macro avg       0.74      0.76      0.74     80843\n",
      "weighted avg       0.77      0.75      0.76     80843\n",
      "\n",
      "\n",
      "XGBoost (TF-IDF+BERT)\n",
      "F1-score: 0.6924198250728864\n",
      "Log loss: 0.5046280051450243\n",
      "Confusion matrix:\n",
      " [[35096 15899]\n",
      " [ 5623 24225]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.69      0.77     50995\n",
      "           1       0.60      0.81      0.69     29848\n",
      "\n",
      "    accuracy                           0.73     80843\n",
      "   macro avg       0.73      0.75      0.73     80843\n",
      "weighted avg       0.77      0.73      0.74     80843\n",
      "\n",
      "\n",
      "RandomForest (TF-IDF+BERT)\n",
      "F1-score: 0.7408400515109285\n",
      "Log loss: 0.4157966333337342\n",
      "Confusion matrix:\n",
      " [[45154  5841]\n",
      " [ 8850 20998]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86     50995\n",
      "           1       0.78      0.70      0.74     29848\n",
      "\n",
      "    accuracy                           0.82     80843\n",
      "   macro avg       0.81      0.79      0.80     80843\n",
      "weighted avg       0.82      0.82      0.82     80843\n",
      "\n",
      "\n",
      "Ensemble (TF-IDF+BERT)\n",
      "F1-score: 0.7355948248658883\n",
      "Log loss: 0.4412084850548564\n",
      "Confusion matrix:\n",
      " [[40774 10221]\n",
      " [ 6537 23311]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.80      0.83     50995\n",
      "           1       0.70      0.78      0.74     29848\n",
      "\n",
      "    accuracy                           0.79     80843\n",
      "   macro avg       0.78      0.79      0.78     80843\n",
      "weighted avg       0.80      0.79      0.79     80843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if hasattr(trainer, \"eval_results\"):\n",
    "    trainer.eval_results.clear()\n",
    "else:\n",
    "    trainer.eval_results = []\n",
    "    \n",
    "# Evaluate all models\n",
    "trainer.evaluate(model_tfidf, X_test_tfidf, y_test, model_name=\"LogReg\", feature_set=\"TF-IDF\")\n",
    "trainer.evaluate(model_combined, X_test_combined, y_test, model_name=\"LogReg\", feature_set=\"TF-IDF+BERT\")\n",
    "trainer.evaluate(model_xgb, X_test_combined, y_test, model_name=\"XGBoost\", feature_set=\"TF-IDF+BERT\")\n",
    "trainer.evaluate(model_rf, X_test_combined, y_test, model_name=\"RandomForest\", feature_set=\"TF-IDF+BERT\")  # ← added RF eval\n",
    "trainer.evaluate(ensemble_model, X_test_combined, y_test, model_name=\"Ensemble\", feature_set=\"TF-IDF+BERT\")\n",
    "\n",
    "# Get results\n",
    "results_df = trainer.summary().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73c46b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Model     Features        F1   LogLoss  \\\n",
      "0        LogReg       TF-IDF  0.661482  0.535475   \n",
      "1  RandomForest       TF-IDF  0.720248  0.457590   \n",
      "2  RandomForest  TF-IDF+BERT  0.740840  0.415797   \n",
      "3        LogReg  TF-IDF+BERT  0.699212  0.494355   \n",
      "4       XGBoost  TF-IDF+BERT  0.692420  0.504628   \n",
      "9      Ensemble  TF-IDF+BERT  0.735595  0.441208   \n",
      "\n",
      "                                               Notes  \n",
      "0        Baseline Logistic Regression, not promising  \n",
      "1                   Baseline RandomForest, promising  \n",
      "2  Random Forest with BERT cosine similarity – th...  \n",
      "3  LogReg with BERT cosine similarity – improved ...  \n",
      "4  XGBoost performs well, especially on positive ...  \n",
      "9  Ensemble is strong, but does not outperform Ra...  \n"
     ]
    }
   ],
   "source": [
    "results_df[\"Notes\"] = [\n",
    "    \"Baseline Logistic Regression, not promising\", \n",
    "    \"Baseline RandomForest, promising\", \n",
    "    \"Random Forest with BERT cosine similarity – the best overall\", \n",
    "    \"LogReg with BERT cosine similarity – improved but not better than RF\",\n",
    "    \"XGBoost performs well, especially on positive class (higher recall)\", \n",
    "    \"Ensemble is strong, but does not outperform Random Forest with BERT\"\n",
    "]\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4d8f58",
   "metadata": {},
   "source": [
    "**Summary:**  \n",
    "Models that use BERT cosine similarity features consistently outperform those that use only TF-IDF. Random Forest with BERT features achieves the best overall balance of precision and recall. While XGBoost achieves higher recall for class 1, Random Forest maintains stronger overall metrics. The ensemble model is competitive, but does not surpass Random Forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c898d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary\n",
    "results_df.to_csv(\"../reports/results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
