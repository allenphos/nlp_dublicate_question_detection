{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fac832d",
   "metadata": {},
   "source": [
    "This notebook tackles class imbalance in the Quora duplicate question detection task by augmenting the training data with high-quality, semantically similar duplicate pairs generated by large language models (LLMs). The augmentation was performed using OpenAI's ChatGPT (gpt-3.5-turbo) and T5 (Text-to-Text Transfer Transformer) to produce paraphrased duplicate question pairs. All synthetic examples were filtered using BERT-based sentence embeddings and cosine similarity to ensure semantic alignment and label quality. The goal was to enrich the dataset with more positive (duplicate) examples, addressing its original class imbalance. About 2,700 high-confidence synthetic duplicates were added. However, the total number was limited due to the high computational and memory demands of generation, embedding, and filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8937b19",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35d1a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import ftfy\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from src.augmentation.llm_augmentation import generate_augmented_pairs\n",
    "from src.features.paraphraser_utils import load_model, batch_paraphrase, is_question\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ad4b52",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ca2ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 1: Passed filter (0.92)\n",
      "✓ 2: Passed filter (0.80)\n",
      "✓ 3: Passed filter (0.94)\n",
      "✓ 4: Passed filter (0.88)\n",
      "✗ 5: Failed filter (0.75)\n",
      "✓ 6: Passed filter (0.76)\n",
      "✗ 7: Failed filter (0.71)\n",
      "✗ 8: Failed filter (0.70)\n",
      "✓ 9: Passed filter (0.79)\n",
      "✓ 10: Passed filter (0.88)\n",
      "✓ 11: Passed filter (0.92)\n",
      "✗ 12: Failed filter (0.72)\n",
      "✓ 13: Passed filter (0.84)\n",
      "✓ 14: Passed filter (0.87)\n",
      "✓ 15: Passed filter (0.88)\n",
      "✗ 16: Duplicate detected, skipping.\n",
      "✓ 17: Passed filter (0.90)\n",
      "✓ 18: Passed filter (0.82)\n",
      "✓ 19: Passed filter (0.80)\n",
      "✓ 20: Passed filter (0.86)\n",
      "✓ 21: Passed filter (0.86)\n",
      "✗ 22: Failed filter (0.75)\n",
      "✓ 23: Passed filter (0.91)\n",
      "✗ 24: Duplicate detected, skipping.\n",
      "✓ 25: Passed filter (0.77)\n",
      "✗ 26: Failed filter (0.60)\n",
      "✓ 27: Passed filter (0.92)\n",
      "✓ 28: Passed filter (0.94)\n",
      "✗ 29: Failed filter (0.69)\n",
      "✓ 30: Passed filter (0.86)\n",
      "✓ 31: Passed filter (0.75)\n",
      "✓ 32: Passed filter (0.75)\n",
      "✓ 33: Passed filter (0.95)\n",
      "✗ 34: Duplicate detected, skipping.\n",
      "✓ 35: Passed filter (0.76)\n",
      "✗ 36: Duplicate detected, skipping.\n",
      "✓ 37: Passed filter (0.87)\n",
      "✓ 38: Passed filter (0.80)\n",
      "✓ 39: Passed filter (0.76)\n",
      "✓ 40: Passed filter (0.87)\n",
      "✓ 41: Passed filter (0.91)\n",
      "✗ 42: Failed filter (0.74)\n",
      "✓ 43: Passed filter (0.84)\n",
      "✗ 44: Duplicate detected, skipping.\n",
      "✗ 45: Failed filter (0.74)\n",
      "✗ 46: Duplicate detected, skipping.\n",
      "✗ 47: Failed filter (0.63)\n",
      "✓ 48: Passed filter (0.87)\n",
      "✓ 49: Passed filter (0.82)\n",
      "✓ 50: Passed filter (0.91)\n",
      "✓ 51: Passed filter (0.90)\n",
      "✓ 52: Passed filter (0.81)\n",
      "✗ 53: Duplicate detected, skipping.\n",
      "✓ 54: Passed filter (0.76)\n",
      "✓ 55: Passed filter (0.93)\n",
      "✓ 56: Passed filter (0.77)\n",
      "✗ 57: Failed filter (0.74)\n",
      "✓ 58: Passed filter (0.96)\n",
      "✗ 59: Failed filter (0.72)\n",
      "✗ 60: Failed filter (0.64)\n",
      "✓ 61: Passed filter (0.94)\n",
      "✗ 62: Failed filter (0.73)\n",
      "✓ 63: Passed filter (0.84)\n",
      "✗ 64: Duplicate detected, skipping.\n",
      "✗ 65: Failed filter (0.65)\n",
      "✗ 66: Duplicate detected, skipping.\n",
      "✓ 67: Passed filter (0.87)\n",
      "✓ 68: Passed filter (0.84)\n",
      "✗ 69: Failed filter (0.67)\n",
      "✓ 70: Passed filter (0.92)\n",
      "✓ 71: Passed filter (0.93)\n",
      "✓ 72: Passed filter (0.82)\n",
      "✓ 73: Passed filter (0.96)\n",
      "✓ 74: Passed filter (0.93)\n",
      "✗ 75: Failed filter (0.66)\n",
      "✗ 76: Duplicate detected, skipping.\n",
      "✓ 77: Passed filter (0.88)\n",
      "✗ 78: Failed filter (0.70)\n",
      "✗ 79: Failed filter (0.69)\n",
      "✓ 80: Passed filter (0.85)\n",
      "✓ 81: Passed filter (0.88)\n",
      "✗ 82: Failed filter (0.74)\n",
      "✓ 83: Passed filter (0.97)\n",
      "✓ 84: Passed filter (0.87)\n",
      "✗ 85: Failed filter (0.65)\n",
      "✓ 86: Passed filter (0.79)\n",
      "✓ 87: Passed filter (0.90)\n",
      "✗ 88: Failed filter (0.69)\n",
      "✓ 89: Passed filter (0.78)\n",
      "✓ 90: Passed filter (0.91)\n",
      "✓ 91: Passed filter (0.94)\n",
      "✗ 92: Duplicate detected, skipping.\n",
      "✓ 93: Passed filter (0.95)\n",
      "✓ 94: Passed filter (0.85)\n",
      "✗ 95: Failed filter (0.65)\n",
      "✗ 96: Duplicate detected, skipping.\n",
      "✗ 97: Failed filter (0.59)\n",
      "✓ 98: Passed filter (0.84)\n",
      "✓ 99: Passed filter (0.83)\n",
      "✓ 100: Passed filter (0.79)\n",
      "✓ 101: Passed filter (0.92)\n",
      "✗ 102: Failed filter (0.72)\n",
      "✓ 103: Passed filter (0.90)\n",
      "✓ 104: Passed filter (0.87)\n",
      "✗ 105: Failed filter (0.66)\n",
      "✗ 106: Failed filter (0.75)\n",
      "✗ 107: Failed filter (0.74)\n",
      "✗ 108: Failed filter (0.73)\n",
      "✓ 109: Passed filter (0.82)\n",
      "✓ 110: Passed filter (0.83)\n",
      "✓ 111: Passed filter (0.87)\n",
      "✗ 112: Failed filter (0.74)\n",
      "✓ 113: Passed filter (0.97)\n",
      "✓ 114: Passed filter (0.86)\n",
      "✓ 115: Passed filter (0.78)\n",
      "✓ 116: Passed filter (0.79)\n",
      "✓ 117: Passed filter (0.95)\n",
      "✓ 118: Passed filter (0.82)\n",
      "✓ 119: Passed filter (0.80)\n",
      "✓ 120: Passed filter (0.94)\n",
      "✓ 121: Passed filter (0.84)\n",
      "✗ 122: Failed filter (0.74)\n",
      "✓ 123: Passed filter (0.96)\n",
      "✗ 124: Duplicate detected, skipping.\n",
      "✗ 125: Failed filter (0.73)\n",
      "✗ 126: Duplicate detected, skipping.\n",
      "✗ 127: Failed filter (0.75)\n",
      "✗ 128: Failed filter (0.70)\n",
      "✗ 129: Failed filter (0.67)\n",
      "✗ 130: Failed filter (0.70)\n",
      "✓ 131: Passed filter (0.88)\n",
      "✗ 132: Failed filter (0.75)\n",
      "✗ 133: Duplicate detected, skipping.\n",
      "✓ 134: Passed filter (0.87)\n",
      "✗ 135: Failed filter (0.74)\n",
      "✓ 136: Passed filter (0.79)\n",
      "✓ 137: Passed filter (0.84)\n",
      "✓ 138: Passed filter (0.83)\n",
      "✓ 139: Passed filter (0.83)\n",
      "✓ 140: Passed filter (0.77)\n",
      "✓ 141: Passed filter (0.93)\n",
      "✗ 142: Failed filter (0.73)\n",
      "✗ 143: Duplicate detected, skipping.\n",
      "✗ 144: Duplicate detected, skipping.\n",
      "✓ 145: Passed filter (0.77)\n",
      "✗ 146: Duplicate detected, skipping.\n",
      "✗ 147: Duplicate detected, skipping.\n",
      "✓ 148: Passed filter (0.76)\n",
      "✗ 149: Failed filter (0.72)\n",
      "✗ 150: Duplicate detected, skipping.\n",
      "✓ 151: Passed filter (0.93)\n",
      "✗ 152: Failed filter (0.73)\n",
      "✓ 153: Passed filter (0.84)\n",
      "✓ 154: Passed filter (0.87)\n",
      "✗ 155: Duplicate detected, skipping.\n",
      "✗ 156: Duplicate detected, skipping.\n",
      "✗ 157: Failed filter (0.73)\n",
      "✓ 158: Passed filter (0.77)\n",
      "✓ 159: Passed filter (0.80)\n",
      "✓ 160: Passed filter (0.76)\n",
      "✓ 161: Passed filter (0.93)\n",
      "✓ 162: Passed filter (0.76)\n",
      "✓ 163: Passed filter (0.94)\n",
      "✓ 164: Passed filter (0.82)\n",
      "✗ 165: Failed filter (0.66)\n",
      "✗ 166: Failed filter (0.72)\n",
      "✗ 167: Failed filter (0.68)\n",
      "✓ 168: Passed filter (0.84)\n",
      "✗ 169: Failed filter (0.73)\n",
      "✓ 170: Passed filter (0.85)\n",
      "✗ 171: Duplicate detected, skipping.\n",
      "✓ 172: Passed filter (0.80)\n",
      "✗ 173: Duplicate detected, skipping.\n",
      "✗ 174: Duplicate detected, skipping.\n",
      "✓ 175: Passed filter (0.92)\n",
      "✓ 176: Passed filter (0.78)\n",
      "✗ 177: Failed filter (0.73)\n",
      "✗ 178: Failed filter (0.73)\n",
      "✓ 179: Passed filter (0.92)\n",
      "✓ 180: Passed filter (0.79)\n",
      "✓ 181: Passed filter (0.87)\n",
      "✗ 182: Failed filter (0.74)\n",
      "✗ 183: Duplicate detected, skipping.\n",
      "✗ 184: Duplicate detected, skipping.\n",
      "✓ 185: Passed filter (0.76)\n",
      "✗ 186: Duplicate detected, skipping.\n",
      "✓ 187: Passed filter (0.78)\n",
      "✗ 188: Failed filter (0.71)\n",
      "✓ 189: Passed filter (0.87)\n",
      "✓ 190: Passed filter (0.79)\n",
      "✓ 191: Passed filter (0.84)\n",
      "✗ 192: Failed filter (0.73)\n",
      "✓ 193: Passed filter (0.93)\n",
      "✓ 194: Passed filter (0.87)\n",
      "✗ 195: Failed filter (0.64)\n",
      "✗ 196: Duplicate detected, skipping.\n",
      "✗ 197: Failed filter (0.66)\n",
      "✓ 198: Passed filter (0.82)\n",
      "✓ 199: Passed filter (0.79)\n",
      "✗ 200: Duplicate detected, skipping.\n",
      "✓ 201: Passed filter (0.89)\n",
      "✗ 202: Duplicate detected, skipping.\n",
      "✗ 203: Duplicate detected, skipping.\n",
      "✗ 204: Duplicate detected, skipping.\n",
      "✓ 205: Passed filter (0.79)\n",
      "✗ 206: Failed filter (0.70)\n",
      "✗ 207: Failed filter (0.66)\n",
      "✓ 208: Passed filter (0.83)\n",
      "✓ 209: Passed filter (0.77)\n",
      "✓ 210: Passed filter (0.84)\n",
      "✓ 211: Passed filter (0.89)\n",
      "✗ 212: Failed filter (0.74)\n",
      "✗ 213: Duplicate detected, skipping.\n",
      "✗ 214: Duplicate detected, skipping.\n",
      "✗ 215: Duplicate detected, skipping.\n",
      "✗ 216: Duplicate detected, skipping.\n",
      "✗ 217: Failed filter (0.57)\n",
      "✗ 218: Duplicate detected, skipping.\n",
      "✗ 219: Failed filter (0.70)\n",
      "✓ 220: Passed filter (0.85)\n",
      "✓ 221: Passed filter (0.92)\n",
      "✗ 222: Failed filter (0.74)\n",
      "✓ 223: Passed filter (0.82)\n",
      "✗ 224: Duplicate detected, skipping.\n",
      "✓ 225: Passed filter (0.76)\n",
      "✗ 226: Failed filter (0.74)\n",
      "✓ 227: Passed filter (0.78)\n",
      "✓ 228: Passed filter (0.75)\n",
      "✗ 229: Failed filter (0.73)\n",
      "✓ 230: Passed filter (0.92)\n",
      "✓ 231: Passed filter (0.88)\n",
      "✗ 232: Duplicate detected, skipping.\n",
      "✗ 233: Duplicate detected, skipping.\n",
      "✓ 234: Passed filter (0.90)\n",
      "✗ 235: Duplicate detected, skipping.\n",
      "✗ 236: Duplicate detected, skipping.\n",
      "✓ 237: Passed filter (0.94)\n",
      "✗ 238: Failed filter (0.74)\n",
      "✗ 239: Failed filter (0.72)\n",
      "✓ 240: Passed filter (0.79)\n",
      "✓ 241: Passed filter (0.89)\n",
      "✗ 242: Failed filter (0.73)\n",
      "✗ 243: Duplicate detected, skipping.\n",
      "✗ 244: Duplicate detected, skipping.\n",
      "✓ 245: Passed filter (0.78)\n",
      "✗ 246: Failed filter (0.70)\n",
      "✗ 247: Duplicate detected, skipping.\n",
      "✗ 248: Failed filter (0.72)\n",
      "✓ 249: Passed filter (0.78)\n",
      "✓ 250: Passed filter (0.76)\n",
      "✓ 251: Passed filter (0.88)\n",
      "✗ 252: Failed filter (0.74)\n",
      "✗ 253: Duplicate detected, skipping.\n",
      "✗ 254: Duplicate detected, skipping.\n",
      "✓ 255: Passed filter (0.93)\n",
      "✗ 256: Duplicate detected, skipping.\n",
      "✓ 257: Passed filter (0.94)\n",
      "✗ 258: Failed filter (0.74)\n",
      "✓ 259: Passed filter (0.84)\n",
      "✓ 260: Passed filter (0.78)\n",
      "✗ 261: Duplicate detected, skipping.\n",
      "✓ 262: Passed filter (0.77)\n",
      "✓ 263: Passed filter (0.86)\n",
      "✗ 264: Duplicate detected, skipping.\n",
      "✗ 265: Failed filter (0.69)\n",
      "✗ 266: Duplicate detected, skipping.\n",
      "✓ 267: Passed filter (0.84)\n",
      "✓ 268: Passed filter (0.76)\n",
      "✗ 269: Failed filter (0.69)\n",
      "✓ 270: Passed filter (0.86)\n",
      "✓ 271: Passed filter (0.88)\n",
      "✗ 272: Failed filter (0.60)\n",
      "✗ 273: Duplicate detected, skipping.\n",
      "✗ 274: Duplicate detected, skipping.\n",
      "✗ 275: Failed filter (0.65)\n",
      "✗ 276: Duplicate detected, skipping.\n",
      "✓ 277: Passed filter (0.89)\n",
      "✗ 278: Duplicate detected, skipping.\n",
      "✗ 279: Failed filter (0.75)\n",
      "✓ 280: Passed filter (0.85)\n",
      "✓ 281: Passed filter (0.87)\n",
      "✗ 282: Duplicate detected, skipping.\n",
      "✗ 283: Duplicate detected, skipping.\n",
      "✗ 284: Duplicate detected, skipping.\n",
      "✗ 285: Failed filter (0.62)\n",
      "✗ 286: Duplicate detected, skipping.\n",
      "✗ 287: Failed filter (0.66)\n",
      "✗ 288: Failed filter (0.71)\n",
      "✓ 289: Passed filter (0.85)\n",
      "✗ 290: Duplicate detected, skipping.\n",
      "✓ 291: Passed filter (0.87)\n",
      "✓ 292: Passed filter (0.86)\n",
      "✓ 293: Passed filter (0.94)\n",
      "✗ 294: Duplicate detected, skipping.\n",
      "✗ 295: Duplicate detected, skipping.\n",
      "✓ 296: Passed filter (0.78)\n",
      "✗ 297: Duplicate detected, skipping.\n",
      "✗ 298: Failed filter (0.74)\n",
      "✓ 299: Passed filter (0.78)\n",
      "✗ 300: Duplicate detected, skipping.\n",
      "✓ 301: Passed filter (0.77)\n",
      "✗ 302: Failed filter (0.74)\n",
      "✗ 303: Duplicate detected, skipping.\n",
      "✗ 304: Duplicate detected, skipping.\n",
      "✗ 305: Failed filter (0.63)\n",
      "✗ 306: Duplicate detected, skipping.\n",
      "✓ 307: Passed filter (0.83)\n",
      "✓ 308: Passed filter (0.82)\n",
      "✓ 309: Passed filter (0.77)\n",
      "✓ 310: Passed filter (0.94)\n",
      "✗ 311: Duplicate detected, skipping.\n",
      "✓ 312: Passed filter (0.76)\n",
      "✗ 313: Duplicate detected, skipping.\n",
      "✓ 314: Passed filter (0.85)\n",
      "✗ 315: Duplicate detected, skipping.\n",
      "✗ 316: Duplicate detected, skipping.\n",
      "✓ 317: Passed filter (0.79)\n",
      "✗ 318: Duplicate detected, skipping.\n",
      "✓ 319: Passed filter (0.76)\n",
      "✗ 320: Duplicate detected, skipping.\n",
      "✓ 321: Passed filter (0.94)\n",
      "✓ 322: Passed filter (0.88)\n",
      "✓ 323: Passed filter (0.93)\n",
      "✗ 324: Duplicate detected, skipping.\n",
      "✓ 325: Passed filter (0.89)\n",
      "✗ 326: Duplicate detected, skipping.\n",
      "✗ 327: Failed filter (0.63)\n",
      "✗ 328: Duplicate detected, skipping.\n",
      "✓ 329: Passed filter (0.80)\n",
      "✓ 330: Passed filter (0.84)\n",
      "✓ 331: Passed filter (0.85)\n",
      "✗ 332: Duplicate detected, skipping.\n",
      "✗ 333: Duplicate detected, skipping.\n",
      "✗ 334: Duplicate detected, skipping.\n",
      "✗ 335: Failed filter (0.72)\n",
      "✗ 336: Duplicate detected, skipping.\n",
      "✗ 337: Failed filter (0.72)\n",
      "✓ 338: Passed filter (0.85)\n",
      "✗ 339: Failed filter (0.75)\n",
      "✓ 340: Passed filter (0.87)\n",
      "✗ 341: Duplicate detected, skipping.\n",
      "✗ 342: Duplicate detected, skipping.\n",
      "✗ 343: Duplicate detected, skipping.\n",
      "✗ 344: Duplicate detected, skipping.\n",
      "✓ 345: Passed filter (0.78)\n",
      "✓ 346: Passed filter (0.77)\n",
      "✓ 347: Passed filter (0.87)\n",
      "✗ 348: Failed filter (0.64)\n",
      "✓ 349: Passed filter (0.76)\n",
      "✓ 350: Passed filter (0.92)\n",
      "✓ 351: Passed filter (0.87)\n",
      "✗ 352: Failed filter (0.74)\n",
      "✗ 353: Duplicate detected, skipping.\n",
      "✗ 354: Duplicate detected, skipping.\n",
      "✗ 355: Failed filter (0.65)\n",
      "✗ 356: Duplicate detected, skipping.\n",
      "✗ 357: Failed filter (0.71)\n",
      "✗ 358: Failed filter (0.75)\n",
      "✗ 359: Failed filter (0.69)\n",
      "✗ 360: Duplicate detected, skipping.\n",
      "✓ 361: Passed filter (0.88)\n",
      "✓ 362: Passed filter (0.78)\n",
      "✓ 363: Passed filter (0.86)\n",
      "✓ 364: Passed filter (0.86)\n",
      "✓ 365: Passed filter (0.84)\n",
      "✗ 366: Duplicate detected, skipping.\n",
      "✗ 367: Failed filter (0.71)\n",
      "✓ 368: Passed filter (0.79)\n",
      "✗ 369: Failed filter (0.72)\n",
      "✓ 370: Passed filter (0.89)\n",
      "✓ 371: Passed filter (0.86)\n",
      "✓ 372: Passed filter (0.93)\n",
      "✗ 373: Duplicate detected, skipping.\n",
      "✗ 374: Duplicate detected, skipping.\n",
      "✗ 375: Failed filter (0.65)\n",
      "✗ 376: Duplicate detected, skipping.\n",
      "✗ 377: Failed filter (0.63)\n",
      "✓ 378: Passed filter (0.83)\n",
      "✓ 379: Passed filter (0.76)\n",
      "✗ 380: Duplicate detected, skipping.\n",
      "✓ 381: Passed filter (0.87)\n",
      "✗ 382: Duplicate detected, skipping.\n",
      "✓ 383: Passed filter (0.95)\n",
      "✗ 384: Duplicate detected, skipping.\n",
      "✗ 385: Duplicate detected, skipping.\n",
      "✗ 386: Duplicate detected, skipping.\n",
      "✓ 387: Passed filter (0.78)\n",
      "✓ 388: Passed filter (0.76)\n",
      "✓ 389: Passed filter (0.85)\n",
      "✓ 390: Passed filter (0.75)\n",
      "✓ 391: Passed filter (0.91)\n",
      "✗ 392: Duplicate detected, skipping.\n",
      "✓ 393: Passed filter (0.80)\n",
      "✓ 394: Passed filter (0.79)\n",
      "✗ 395: Failed filter (0.72)\n",
      "✗ 396: Failed filter (0.72)\n",
      "✗ 397: Failed filter (0.65)\n",
      "✗ 398: Failed filter (0.71)\n",
      "✓ 399: Passed filter (0.79)\n",
      "✗ 400: Duplicate detected, skipping.\n",
      "✓ 401: Passed filter (0.88)\n",
      "✓ 402: Passed filter (0.84)\n",
      "✗ 403: Duplicate detected, skipping.\n",
      "✗ 404: Duplicate detected, skipping.\n",
      "✓ 405: Passed filter (0.76)\n",
      "✗ 406: Duplicate detected, skipping.\n",
      "✓ 407: Passed filter (0.83)\n",
      "✓ 408: Passed filter (0.83)\n",
      "✓ 409: Passed filter (0.87)\n",
      "✓ 410: Passed filter (0.80)\n",
      "✓ 411: Passed filter (0.86)\n",
      "✗ 412: Failed filter (0.74)\n",
      "✓ 413: Passed filter (0.93)\n",
      "✗ 414: Duplicate detected, skipping.\n",
      "✓ 415: Passed filter (0.78)\n",
      "✗ 416: Duplicate detected, skipping.\n",
      "✗ 417: Failed filter (0.70)\n",
      "✓ 418: Passed filter (0.77)\n",
      "✗ 419: Failed filter (0.65)\n",
      "✗ 420: Duplicate detected, skipping.\n",
      "✓ 421: Passed filter (0.92)\n",
      "✗ 422: Failed filter (0.73)\n",
      "✗ 423: Duplicate detected, skipping.\n",
      "✗ 424: Duplicate detected, skipping.\n",
      "✗ 425: Duplicate detected, skipping.\n",
      "✗ 426: Duplicate detected, skipping.\n",
      "✓ 427: Passed filter (0.90)\n",
      "✓ 428: Passed filter (0.78)\n",
      "✓ 429: Passed filter (0.82)\n",
      "✗ 430: Duplicate detected, skipping.\n",
      "✗ 431: Duplicate detected, skipping.\n",
      "✗ 432: Failed filter (0.73)\n",
      "✗ 433: Duplicate detected, skipping.\n",
      "✓ 434: Passed filter (0.93)\n",
      "✗ 435: Failed filter (0.65)\n",
      "✗ 436: Duplicate detected, skipping.\n",
      "✗ 437: Failed filter (0.75)\n",
      "✗ 438: Failed filter (0.74)\n",
      "✓ 439: Passed filter (0.84)\n",
      "✗ 440: Duplicate detected, skipping.\n",
      "✓ 441: Passed filter (0.92)\n",
      "✗ 442: Failed filter (0.73)\n",
      "✗ 443: Duplicate detected, skipping.\n",
      "✓ 444: Passed filter (0.88)\n",
      "✗ 445: Duplicate detected, skipping.\n",
      "✓ 446: Passed filter (0.78)\n",
      "✓ 447: Passed filter (0.83)\n",
      "✗ 448: Duplicate detected, skipping.\n",
      "✗ 449: Failed filter (0.69)\n",
      "✓ 450: Passed filter (0.86)\n",
      "✓ 451: Passed filter (0.92)\n",
      "✗ 452: Failed filter (0.57)\n",
      "✗ 453: Duplicate detected, skipping.\n",
      "✓ 454: Passed filter (0.78)\n",
      "✗ 455: Failed filter (0.65)\n",
      "✗ 456: Duplicate detected, skipping.\n",
      "✗ 457: Duplicate detected, skipping.\n",
      "✓ 458: Passed filter (0.87)\n",
      "✗ 459: Failed filter (0.75)\n",
      "✓ 460: Passed filter (0.92)\n",
      "✓ 461: Passed filter (0.83)\n",
      "✗ 462: Duplicate detected, skipping.\n",
      "✓ 463: Passed filter (0.93)\n",
      "✗ 464: Duplicate detected, skipping.\n",
      "✗ 465: Failed filter (0.64)\n",
      "✗ 466: Failed filter (0.70)\n",
      "✓ 467: Passed filter (0.93)\n",
      "✗ 468: Duplicate detected, skipping.\n",
      "✗ 469: Failed filter (0.69)\n",
      "✓ 470: Passed filter (0.92)\n",
      "✓ 471: Passed filter (0.85)\n",
      "✗ 472: Failed filter (0.73)\n",
      "✓ 473: Passed filter (0.85)\n",
      "✓ 474: Passed filter (0.93)\n",
      "✓ 475: Passed filter (0.79)\n",
      "✗ 476: Duplicate detected, skipping.\n",
      "✓ 477: Passed filter (0.87)\n",
      "✗ 478: Failed filter (0.70)\n",
      "✓ 479: Passed filter (0.87)\n",
      "✓ 480: Passed filter (0.80)\n",
      "✓ 481: Passed filter (0.93)\n",
      "✗ 482: Duplicate detected, skipping.\n",
      "✗ 483: Duplicate detected, skipping.\n",
      "✗ 484: Duplicate detected, skipping.\n",
      "✗ 485: Duplicate detected, skipping.\n",
      "✗ 486: Duplicate detected, skipping.\n",
      "✗ 487: Failed filter (0.66)\n",
      "✓ 488: Passed filter (0.83)\n",
      "✗ 489: Failed filter (0.67)\n",
      "✗ 490: Duplicate detected, skipping.\n",
      "✓ 491: Passed filter (0.81)\n",
      "✓ 492: Passed filter (0.78)\n",
      "✗ 493: Duplicate detected, skipping.\n",
      "✗ 494: Duplicate detected, skipping.\n",
      "✗ 495: Failed filter (0.65)\n",
      "✗ 496: Duplicate detected, skipping.\n",
      "✓ 497: Passed filter (0.85)\n",
      "✗ 498: Failed filter (0.73)\n",
      "✗ 499: Failed filter (0.73)\n",
      "✗ 500: Duplicate detected, skipping.\n",
      "Saved to augmented_duplicate_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "# Load OpenAI API key\n",
    "with open(\"../creds.json\") as f:\n",
    "    creds = json.load(f)\n",
    "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Initialize LLM and embeddings\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.4)\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"theme\"],\n",
    "    template=(\n",
    "        \"Generate an original natural-sounding user question on the topic of \\\"{theme}\\\", \"\n",
    "        \"followed by a semantically equivalent version of the same question with different wording. Use this format:\\n\\n\"\n",
    "        \"Q1: <original question>\\n\"\n",
    "        \"Q2: <paraphrased version>\\n\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "themes = [\n",
    "    \"travel\", \"finance\", \"health\", \"technology\", \"education\",\n",
    "    \"job search\", \"taxes\", \"e-commerce\", \"cybersecurity\", \"food\"\n",
    "]\n",
    "\n",
    "df_aug = generate_augmented_pairs(\n",
    "    themes,\n",
    "    n_pairs=500,               \n",
    "    similarity_threshold=0.75,\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    delay=1.5\n",
    ")\n",
    "df_aug.to_csv(\"../data/processed/augmented_duplicate_pairs.csv\", index=False)\n",
    "print(\"Saved to augmented_duplicate_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1853a1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making another 500 duplicate pairs \n",
    "df_aug2 = generate_augmented_pairs(\n",
    "    themes,\n",
    "    n_pairs=1000,               \n",
    "    similarity_threshold=0.75,\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    delay=1.5\n",
    ")\n",
    "df_aug2.to_csv(\"../data/processed/2augmented_duplicate_pairs.csv\", index=False)\n",
    "print(\"Saved to augmented_duplicate_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17de8307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned dataset with 570 pairs saved.\n"
     ]
    }
   ],
   "source": [
    "# Cheching for dublicates between 2 files and combine them\n",
    "combined_df = pd.concat([df_aug, df_aug2], ignore_index=True)\n",
    "combined_df.drop_duplicates(subset=[\"question1\", \"question2\"], inplace=True)\n",
    "\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "combined_df.to_csv(\"../data/processed/augmented_duplicate_pairs_500.csv\", index=False)\n",
    "print(\"Final cleaned dataset with\", len(combined_df), \"pairs saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aaf71c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to main training set\n",
    "df_train = pd.read_csv(\"../data/processed/quora_train_cleaned.csv\")\n",
    "combined_df = pd.read_csv(\"../data/processed/augmented_duplicate_pairs_500.csv\")\n",
    "\n",
    "# Concatenate\n",
    "df_full = pd.concat([df_train, combined_df], ignore_index=True)\n",
    "\n",
    "# Remove accidental repeats (handle (q1, q2) and (q2, q1) as duplicates)\n",
    "df_full['q1_q2'] = df_full.apply(\n",
    "    lambda row: \" || \".join(sorted([row['question1'], row['question2']])), axis=1\n",
    ")\n",
    "df_full = df_full.drop_duplicates(subset=['q1_q2'])\n",
    "df_full = df_full.drop(columns=['q1_q2'])\n",
    "\n",
    "# Shuffle \n",
    "df_full = df_full.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce08a15",
   "metadata": {},
   "source": [
    "# T5 Paraphraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a1c967e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [15:32<00:00,  1.49s/it]\n",
      "100%|██████████| 625/625 [15:43<00:00,  1.51s/it]\n",
      "100%|██████████| 20000/20000 [00:06<00:00, 3326.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model \n",
    "tokenizer, model, device = load_model()\n",
    "\n",
    "# Sample subset\n",
    "n_to_augment = 20000\n",
    "train_sample = df_full.sample(n=n_to_augment, random_state=42).dropna(subset=['question1', 'question2']).reset_index(drop=True)\n",
    "\n",
    "# Paraphrase\n",
    "q1_paraphrased = batch_paraphrase(train_sample['question1'].tolist(), tokenizer, model, device)\n",
    "q2_paraphrased = batch_paraphrase(train_sample['question2'].tolist(), tokenizer, model, device)\n",
    "\n",
    "assert len(train_sample) == len(q1_paraphrased) == len(q2_paraphrased), \"Mismatch in lengths!\"\n",
    "\n",
    "# Create directory for checkpoints\n",
    "checkpoint_dir = \"paraphrase_checkpoints2\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Augment and save\n",
    "augmented_rows = []\n",
    "\n",
    "for i in tqdm(range(len(train_sample))):\n",
    "    row = train_sample.iloc[i].copy()\n",
    "    \n",
    "    # Partial paraphrase\n",
    "    row1 = row.copy(); row1['question1'] = q1_paraphrased[i]; augmented_rows.append(row1)\n",
    "    row2 = row.copy(); row2['question2'] = q2_paraphrased[i]; augmented_rows.append(row2)\n",
    "\n",
    "    # Only positive duplicates fully paraphrased\n",
    "    if row['is_duplicate'] == 1:\n",
    "        row3 = row.copy()\n",
    "        row3['question1'] = q1_paraphrased[i]\n",
    "        row3['question2'] = q2_paraphrased[i]\n",
    "        augmented_rows.append(row3)\n",
    "\n",
    "    # Save in chunks\n",
    "    if (i + 1) % 3000 == 0 or (i + 1) == len(train_sample):\n",
    "        checkpoint_df = pd.DataFrame(augmented_rows)\n",
    "        checkpoint_df = checkpoint_df[\n",
    "            checkpoint_df['question1'].apply(is_question) & checkpoint_df['question2'].apply(is_question)\n",
    "        ].drop_duplicates(subset=['question1', 'question2', 'is_duplicate']).reset_index(drop=True)\n",
    "\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"augmented_checkpoint2_{i+1}.csv\")\n",
    "        checkpoint_df.to_csv(checkpoint_path, index=False)\n",
    "        augmented_rows = []  # Clear buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51113e0d",
   "metadata": {},
   "source": [
    "# Preprocessing of the augmented data and merging with the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef7381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded augmented rows: 5197\n",
      "Loaded extra file: 570\n",
      "Total combined augmented rows: 5767\n",
      "After NaN removal: 5767\n",
      "Kept only positive duplicates: 2701\n",
      "Duplicate (Q1, Q2) pairs in augmented_df: 50\n",
      "After English filtering: 2693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2693/2693 [00:47<00:00, 56.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated q-pairs (ignoring label) deleted: 100\n",
      "✅ Saved augmented and full dataset.\n"
     ]
    }
   ],
   "source": [
    "# Load and concatenate all augmented CSV files\n",
    "augmented_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
    "print(f\"Loaded augmented rows: {len(augmented_df)}\")\n",
    "\n",
    "# Load additional 500 duplicates file\n",
    "combined_df = pd.read_csv(\"../data/processed/augmented_duplicate_pairs_500.csv\")\n",
    "print(f\"Loaded extra file: {len(combined_df)}\")\n",
    "\n",
    "# 3. Combine all augmented data\n",
    "augmented_df = pd.concat([augmented_df, combined_df], ignore_index=True)\n",
    "print(f\"Total combined augmented rows: {len(augmented_df)}\")\n",
    "\n",
    "augmented_df = augmented_df.dropna(subset=['question1', 'question2']).reset_index(drop=True)\n",
    "print(f\"After NaN removal: {len(augmented_df)}\")\n",
    "augmented_df = augmented_df[augmented_df['is_duplicate'] == 1].reset_index(drop=True)\n",
    "print(f\"Kept only positive duplicates: {len(augmented_df)}\")\n",
    "dup_count = augmented_df.duplicated(subset=['question1', 'question2']).sum()\n",
    "print(f\"Duplicate (Q1, Q2) pairs in augmented_df: {dup_count}\")\n",
    "\n",
    "\n",
    "# Function to clean text: fix encoding, lowercase, trim whitespace\n",
    "def preprocess_augmented(text):\n",
    "    text = str(text)\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to both questions\n",
    "augmented_df['question1'] = augmented_df['question1'].apply(preprocess_augmented)\n",
    "augmented_df['question2'] = augmented_df['question2'].apply(preprocess_augmented)\n",
    "\n",
    "# Keep only English rows based on ASCII check\n",
    "def is_english(text):\n",
    "    try:\n",
    "        text.encode('utf-8').decode('ascii')\n",
    "        return True\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "\n",
    "augmented_df = augmented_df[\n",
    "    augmented_df['question1'].apply(is_english) &\n",
    "    augmented_df['question2'].apply(is_english)\n",
    "].reset_index(drop=True)\n",
    "print(f\"After English filtering: {len(augmented_df)}\")\n",
    "\n",
    "# Recalculate `is_duplicate` using semantic similarity\n",
    "sim_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def recalculate_duplicate_label(q1, q2, threshold=0.8):\n",
    "    emb1 = sim_model.encode(q1, convert_to_tensor=True)\n",
    "    emb2 = sim_model.encode(q2, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(emb1, emb2)\n",
    "    return 1 if similarity.item() > threshold else 0\n",
    "\n",
    "# Apply label recalculation with progress bar\n",
    "augmented_df['is_duplicate'] = [\n",
    "    recalculate_duplicate_label(q1, q2)\n",
    "    for q1, q2 in tqdm(zip(augmented_df['question1'], augmented_df['question2']), total=len(augmented_df))\n",
    "]\n",
    "# Remove NaNs\n",
    "augmented_df = augmented_df.dropna(subset=['question1', 'question2']).reset_index(drop=True)\n",
    "\n",
    "# Keep only positive duplicates\n",
    "augmented_df = augmented_df[augmented_df['is_duplicate'] == 1].reset_index(drop=True)\n",
    "\n",
    "# Check for q1-q2 duplicates (ignoring label)\n",
    "dup_count = augmented_df.duplicated(subset=['question1', 'question2']).sum()\n",
    "print(f\"Duplicated q-pairs (ignoring label) deleted: {dup_count}\")\n",
    "\n",
    "# Then drop true duplicates\n",
    "augmented_df = augmented_df.drop_duplicates(subset=['question1', 'question2', 'is_duplicate'])\n",
    "\n",
    "# Merge with original dataset and remove exact duplicates\n",
    "full_df = pd.concat([df_full, augmented_df], ignore_index=True).drop_duplicates(\n",
    "    subset=['question1', 'question2', 'is_duplicate']\n",
    ").sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save the cleaned and combined datasets\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "augmented_df.to_csv(\"../data/processed/augmented_questions.csv\", index=False)\n",
    "full_df.to_csv(\"../data/processed/full_train_augmented.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved augmented and full dataset.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13fc9e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180 324649\n"
     ]
    }
   ],
   "source": [
    "print(len(augmented_df), len(full_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b93d89fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: have you ever seen someone die?\n",
      "Q2: have you ever seen anyone die?\n",
      "Is Duplicate: 1\n",
      "---\n",
      "Q1: what's the other thing you think about your mom?\n",
      "Q2: what's that one thing that comes to your mind when you think about your mother?\n",
      "Is Duplicate: 1\n",
      "---\n",
      "Q1: what are best books for ssc cgl?\n",
      "Q2: are there any books for ssc cgl?\n",
      "Is Duplicate: 1\n",
      "---\n",
      "Q1: why do i like girls soles?\n",
      "Q2: i like girls bare soles?\n",
      "Is Duplicate: 1\n",
      "---\n",
      "Q1: i know this is known question but how do i know if she likes me?\n",
      "Q2: how would i know if she really likes me?\n",
      "Is Duplicate: 1\n",
      "---\n",
      "Q1: does cannabis oil cure cancer?\n",
      "Q2: how does cannabis affect cancer?\n",
      "Is Duplicate: 1\n",
      "---\n",
      "Q1: how can one prepare for entrance exam of top ibdp schools?\n",
      "Q2: how to prepare yourself for the ibdp school entrance exam?\n",
      "Is Duplicate: 1\n",
      "---\n",
      "Q1: what all career options i have after ece?\n",
      "Q2: career advice - i have done my be ece, what are the various career options for me now?\n",
      "Is Duplicate: 1\n",
      "---\n",
      "Q1: my google account is disabled. how can i enable it?\n",
      "Q2: how do you enable a disabled google account?\n",
      "Is Duplicate: 1\n",
      "---\n",
      "Q1: how can i deal with my toxic mother?\n",
      "Q2: how do i deal with my toxic mother?\n",
      "Is Duplicate: 1\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i, row in augmented_df[10:20].iterrows():\n",
    "    print(\"Q1:\", row[\"question1\"])\n",
    "    print(\"Q2:\", row[\"question2\"])\n",
    "    print(\"Is Duplicate:\", row[\"is_duplicate\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8280ce27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "is_duplicate\n",
      "0    0.626843\n",
      "1    0.373157\n",
      "Name: proportion, dtype: float64\n",
      "is_duplicate\n",
      "0    0.63024\n",
      "1    0.36976\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check new class balance\n",
    "print(\"Class distribution:\")\n",
    "print(full_df[\"is_duplicate\"].value_counts(normalize=True))\n",
    "print(df_train[\"is_duplicate\"].value_counts(normalize=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
