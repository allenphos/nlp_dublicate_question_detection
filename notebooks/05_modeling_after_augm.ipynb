{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac72284",
   "metadata": {},
   "source": [
    "This notebook addresses class imbalance in the Quora duplicate question detection task by augmenting the training dataset with approximately 2,700 additional duplicate question pairs generated using a language model. All synthetic pairs were filtered using BERT-based semantic similarity to ensure label consistency and diversity. The final dataset was vectorized using TF-IDF features, and semantic similarity scores were computed using BERT embeddings. Random Forest was selected as the primary model due to its robust performance on the base dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "import re\n",
    "import ftfy\n",
    "\n",
    "from src.features.similarity import TextEmbedder, compute_and_save_bert_features\n",
    "import joblib\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from src.models.trainers import ClassicMLTrainer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8073b0f",
   "metadata": {},
   "source": [
    "# BERT embeddings and cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc9d9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique questions: 451315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT embeddings: 100%|██████████| 7052/7052 [42:31<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../data/processed/aug_quora_train_embeddings.npy, shape: (451315, 768)\n",
      "Saved ../data/processed/aug_quora_train_with_bert_sim.csv and .npy embeddings.\n"
     ]
    }
   ],
   "source": [
    "df_full = pd.read_csv(\"../data/processed/full_train_augmented.csv\", index_col=False)\n",
    "\n",
    "embedder = TextEmbedder(model_name=\"bert-base-uncased\")\n",
    "compute_and_save_bert_features(df_full, embedder, \"../data/processed/aug_quora_train\", batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e879eef",
   "metadata": {},
   "source": [
    "# Prepare features for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acaa5603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set (with precomputed cosine and cleaned text)\n",
    "test_df = pd.read_csv(\"../data/processed/quora_test_with_bert_sim.csv\")\n",
    "\n",
    "# 3. Load vectorizer\n",
    "tfidf_vectorizer = joblib.load(\"../src/models/tfidf_vectorizer.joblib\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "df_full[\"cleaned_text\"] = (df_full[\"question1\"] + \" [SEP] \" + df_full[\"question2\"]).apply(preprocess_text)\n",
    "\n",
    "df_full = df_full.dropna(subset=[\"question1\", \"question2\"]).reset_index(drop=True)\n",
    "df_full[\"question1\"] = df_full[\"question1\"].astype(str)\n",
    "df_full[\"question2\"] = df_full[\"question2\"].astype(str)\n",
    "\n",
    "test_df = test_df.dropna(subset=[\"question1\", \"question2\"]).reset_index(drop=True)\n",
    "test_df[\"question1\"] = test_df[\"question1\"].astype(str)\n",
    "test_df[\"question2\"] = test_df[\"question2\"].astype(str)\n",
    "\n",
    "test_df[\"cleaned_text\"] = (test_df[\"question1\"] + \" [SEP] \" + test_df[\"question2\"]).apply(preprocess_text)\n",
    "\n",
    "# Transform cleaned text columns to TF-IDF feature matrices\n",
    "X_train_tfidf = tfidf_vectorizer.transform(df_full[\"cleaned_text\"])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df[\"cleaned_text\"])\n",
    "\n",
    "# 5. Add cosine similarity column\n",
    "X_train_cos = df_full[[\"bert_cosine_similarity\"]].values\n",
    "X_test_cos = test_df[[\"bert_cosine_similarity\"]].values\n",
    "\n",
    "# 6. Combine TF-IDF and BERT cosine into final feature set\n",
    "X_train_combined = hstack([X_train_tfidf, X_train_cos])\n",
    "X_test_combined = hstack([X_test_tfidf, X_test_cos])\n",
    "\n",
    "# 7. Target values\n",
    "y_train = df_full[\"is_duplicate\"].values\n",
    "y_test = test_df[\"is_duplicate\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55efb00",
   "metadata": {},
   "source": [
    "# Train classical ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b59aedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForest (TF-IDF)\n",
      "F1-score: 0.6777985279578161\n",
      "Log loss: 0.5274705664285126\n",
      "Confusion matrix:\n",
      " [[44750  6255]\n",
      " [11343 18510]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84     51005\n",
      "           1       0.75      0.62      0.68     29853\n",
      "\n",
      "    accuracy                           0.78     80858\n",
      "   macro avg       0.77      0.75      0.76     80858\n",
      "weighted avg       0.78      0.78      0.78     80858\n",
      "\n",
      "Feature 7812: 0.0335\n",
      "Feature 3451: 0.0247\n",
      "Feature 1042: 0.0208\n",
      "Feature 708: 0.0175\n",
      "Feature 5690: 0.0110\n",
      "Feature 652: 0.0084\n",
      "Feature 2886: 0.0080\n",
      "Feature 3481: 0.0073\n",
      "Feature 3389: 0.0073\n",
      "Feature 7850: 0.0068\n",
      "\n",
      "RandomForest (TF-IDF+BERT)\n",
      "F1-score: 0.7045192835757437\n",
      "Log loss: 0.4963457345884709\n",
      "Confusion matrix:\n",
      " [[44295  6710]\n",
      " [ 9969 19884]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.84     51005\n",
      "           1       0.75      0.67      0.70     29853\n",
      "\n",
      "    accuracy                           0.79     80858\n",
      "   macro avg       0.78      0.77      0.77     80858\n",
      "weighted avg       0.79      0.79      0.79     80858\n",
      "\n",
      "Feature 8000: 0.1806\n",
      "Feature 7812: 0.0268\n",
      "Feature 3451: 0.0190\n",
      "Feature 1042: 0.0167\n",
      "Feature 708: 0.0144\n",
      "Feature 5690: 0.0085\n",
      "Feature 652: 0.0069\n",
      "Feature 2886: 0.0065\n",
      "Feature 3481: 0.0061\n",
      "Feature 3389: 0.0059\n"
     ]
    }
   ],
   "source": [
    "trainer = ClassicMLTrainer(n_jobs=-1)\n",
    "\n",
    "# Random Forest: TF-IDF only\n",
    "model_rf_tfidf = trainer.train_rf_tfidf(X_train_tfidf, y_train)\n",
    "trainer.evaluate(model_rf_tfidf, X_test_tfidf, y_test, model_name=\"RandomForest\", feature_set=\"TF-IDF\")\n",
    "trainer.feature_importance(model_rf_tfidf, tfidf_vectorizer)\n",
    "\n",
    "\n",
    "# Random Forest: TF-IDF + BERT\n",
    "model_rf = trainer.train_rf_combined(X_train_combined, y_train)\n",
    "trainer.evaluate(model_rf, X_test_combined, y_test, model_name=\"RandomForest\", feature_set=\"TF-IDF+BERT\")\n",
    "trainer.feature_importance(model_rf, tfidf_vectorizer, feature_names_extra=[\"bert_cosine_similarity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f5fcfa",
   "metadata": {},
   "source": [
    "# Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73c828fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models\n",
    "trainer.models[\"rf_combined\"] = model_rf  \n",
    "trainer.models[\"rf_tfidf\"] = model_rf_tfidf \n",
    "\n",
    "trainer.save_model(\"rf_combined\", \"../src/models/rf_combined.joblib\") \n",
    "trainer.save_model(\"rf_tfidf\", \"../src/models/rf_tfidf.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f32d0",
   "metadata": {},
   "source": [
    "# Summary table of results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca882ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(trainer, \"eval_results\"):\n",
    "    trainer.eval_results.clear()\n",
    "else:\n",
    "    trainer.eval_results = []\n",
    "    \n",
    "# Evaluate all models\n",
    "trainer.evaluate(model_rf_tfidf, X_test_tfidf, y_test, model_name=\"RandomForest\", feature_set=\"TF-IDF\")\n",
    "trainer.evaluate(model_rf, X_test_combined, y_test, model_name=\"RandomForest\", feature_set=\"TF-IDF+BERT\")  # ← added RF eval\n",
    "\n",
    "# Get results\n",
    "results_df_augm = trainer.summary().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54458e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Model     Features        F1   LogLoss  \\\n",
      "0  RandomForest       TF-IDF  0.677799  0.527471   \n",
      "1  RandomForest  TF-IDF+BERT  0.704519  0.496346   \n",
      "\n",
      "                                               Notes  \n",
      "0                    Baseline RandomForest get worse  \n",
      "1  Random Forest with BERT cosine similarity – ge...  \n"
     ]
    }
   ],
   "source": [
    "results_df_augm[\"Notes\"] = [\n",
    "    \"Baseline RandomForest get worse\", \n",
    "    \"Random Forest with BERT cosine similarity – get worse\", \n",
    "]\n",
    "\n",
    "print(results_df_augm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a452ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary\n",
    "results_df_augm.to_csv(\"../reports/results_augm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421eff7",
   "metadata": {},
   "source": [
    "Despite the intended improvements from data augmentation, the overall model performance declined. The Random Forest model trained on the augmented dataset showed reduced F1-score and higher log loss compared to the version trained on the original dataset. This drop in quality may be attributed to the relatively low semantic fidelity of some augmented duplicate pairs, which introduced noise rather than improving class balance. The experiment suggests that while data augmentation can address class imbalance, the quality of synthetic examples plays a critical role in maintaining or improving model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
